#template = """Given the following conversation respond to the best of your ability in a pirate voice and end every sentence with Ay Ay Matey
#Chat History:
#{chat_history}
#Follow Up Input: {question}
#Standalone question:"""

#PROMPT = PromptTemplate(
#    input_variables=["chat_history", "question"], 
#    template=template
#)

chain = ConversationalRetrievalChain.from_llm(llm=llm, chain_type='stuff',
                                                retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
                                                memory=memory)
                                                #combine_docs_chain_kwargs={"prompt": "Question: "})






#llm_model= st.sidebar.selectbox("LLM model for generating responses", ["gpt-3.5-turbo on OpenAI", "gpt-4 on OpenAI", "llama-2-70b-chat on Replicate"], index=0)


from langchain.llms import CTransformers
from langchain.chat_models import ChatOpenAI

#from dotenv import load_dotenv
#load_dotenv()



"""
from langchain.llms.fake import FakeListLLM
from langchain.llms.human import HumanInputLLM
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings.fake import DeterministicFakeEmbedding, FakeEmbeddings
from langchain.vectorstores import Chroma


llm = FakeListLLM(responses=["Response"])
response = llm("Question?", verbose=True)
print(response)

llm = HumanInputLLM(
    prompt_func=lambda prompt: print(
        f"\n====PROMPT====\n{prompt}\n====END OF PROMPT===="
    )
)
response = llm("Question?", verbose=True)
print(response)


loader = CSVLoader(file_path='oscar_age_male.csv')
data = loader.load()
# data is list of documents
# len(data) is the number of rows in the csv file (header is not counted)
# data[0] is Document(page_content='Index: 1\nYear: 1928\nAge: 44\nName: Emil Jannings\nMovie: The Last Command, The Way of All Flesh', metadata={'source': 'oscar_age_male.csv', 'row': 0})
# data[0].page_content is string with each cell separated by \n
# data[0].metadata is dict with keys 'source' and 'row'

db = Chroma.from_documents(data, FakeEmbeddings(size=128))

query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
print(docs[0])

"""

"""
from io import BytesIO
import os
import time

import streamlit as st
import pandas as pd
from langchain.document_loaders import UnstructuredExcelLoader, UnstructuredWordDocumentLoader, PyPDFLoader, TextLoader
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from PyPDF2 import PdfReader

# pip install pypdf --> pip install PyPDF2
# pip install docx2txt

st.sidebar.header("Settings")

openai_api_key = st.sidebar.text_input("OpenAI API key", type="password")
os.environ["OPENAI_API_KEY"] = openai_api_key

name = st.sidebar.text_input("Name", "My Assistant")

model= st.sidebar.selectbox("Model", ["davinci", "curie", "babbage", "ada"], index=0)

tone = st.sidebar.selectbox("Tone", ["Formal", "Informal"], index=0)

role = st.sidebar.text_area("Role", "Description of assistant responsibilities and scope of work")

audience = st.sidebar.text_area("Audience", "Description of wider audience and most likely consumers of the content")

@st.cache_data
def load_file(file):
    time.sleep(1)
    docs = []
    data_to_display = None
    if file.name.endswith(".txt"):
        data_to_display = file.read().decode()
        #text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.create_documents([data_to_display])
        #st.write(docs)
        #assert len(docs) == 1

        #embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        #db = Chroma.from_documents(texts, embeddings)
        #retriever = db.as_retriever()
        #qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)
        #return qa.run(query_text)

        #loader = TextLoader(file)
        #docs = loader.load()
    elif file.name.endswith(".csv"):
        pass
        #loader = CSVLoader(file.read().decode())
        #docs = loader.load()

        # FIXME: This.
        docs = [
            Document(page_content="", metadata={"source": file.name})  #  "row": 0
        ]   # , "row": 0}}}]


        data_to_display = pd.read_csv(file)
        #st.write(data_to_display.head())
    elif file.name.endswith(".xlsx") or file.name.endswith(".xls"):
        pass
        #loader = UnstructuredExcelLoader(file)
        #docs = loader.load()
    elif file.name.endswith(".docx") or file.name.endswith(".doc"):
        pass
        #loader = UnstructuredWordDocumentLoader(file)
        #docs = loader.load()
    elif file.name.endswith(".pdf"):
        pass

        bytes_stream = BytesIO(file.read())
        reader = PdfReader(bytes_stream)

        # FIXME: This.
        docs = [
            Document(page_content="", metadata={"source": file.name}) 
        ]

        data_to_display = ""
        for page in reader.pages:
            data_to_display += page.extract_text()

        #loader = PyPDFLoader(file.name)
        #docs = loader.load_and_split()
    else:
        #st.toast("File type not supported")
        pass
    #st.write(len(docs))
    #st.write(type(docs))

    for doc in docs:
        doc.metadata["source"] = file.name

    return docs, data_to_display


def load_files(files):
    doc_list_list = []
    data_list_to_display = []
    for file in files:
        doc_list, data_to_display = load_file(file)
        doc_list_list.append(doc_list)
        data_list_to_display.append(data_to_display)
    return doc_list_list, data_list_to_display

def upload():
    if len(files) == 0:
        return [], []
    
    with st.spinner("File is being processed..." if len(files) == 1 else "Files are being processed..."):
        data = load_files(files)
    st.toast("File is ready." if len(files) == 1 else "Files are ready.")
    # st.write("filename:", file.name)
    return data


files = st.sidebar.file_uploader("Knowledge", accept_multiple_files=True, type=["txt", "pdf", "docx", "doc", "csv", "xlsx", "xls"])

st.header(name)

tab1, tab2 = st.tabs(["Ask questions", "Display knowledge"])

with tab1:
    message = st.text_area("Your question:", height=100, max_chars=1000)
    st.button("Ask", type="primary")

doc_list_list, data_list_to_display = upload()

with tab2:
    if len(doc_list_list) > 0:
        tabs = st.tabs([doc_list[0].metadata["source"] for doc_list in doc_list_list])
        for tab, data_to_display in zip(tabs, data_list_to_display):
            with tab:
                if isinstance(data_to_display, str):
                    st.write(data_to_display)
                elif isinstance(data_to_display, pd.DataFrame):
                    st.dataframe(data_to_display)
                    #st.write(data_to_display.head())
                else:
                    assert False
"""

"""
loader = UnstructuredExcelLoader("example_data/stanley-cups.xlsx", mode="elements")
docs = loader.load()
docs[0]

loader = CSVLoader(file_path="./example_data/mlb_teams_2012.csv")
data = loader.load()

# df = pd.read_csv("example_data/mlb_teams_2012.csv")

loader = UnstructuredWordDocumentLoader("example_data/fake.docx")
data = loader.load()

loader = PyPDFLoader("example_data/layout-parser-paper.pdf")
pages = loader.load_and_split()
"""


"""
    data = []
    for file in uploaded_files:

    return data
"""
